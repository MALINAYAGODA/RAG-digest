Группа вопросов/тема,Вопрос,Ответ на вопрос,Сложность/уровень вопроса,"Попадался ли на интервью, если да то какой компании",Ссылка на статью с объяснением/ответом
¯\_(ツ)_/¯,Нормализация распределения-1. Приведите методы преобразования ненормального распределения к нормальному,"Нормализация распределения данных используется для приведения данных к определенному диапазону или распределению. Основные методы нормализации включают:

Min-Max нормализация: Приводит данные к заданному диапазону, обычно [0, 1]. Применяется, когда необходимо сохранить отношения между исходными данными.
Z-score нормализация (стандартизация): Преобразует данные так, чтобы они имели среднее значение 0 и стандартное отклонение 1. Используется, когда необходимо устранить влияние масштаба.
Логарифмическое преобразование: Применяется для уменьшения асимметрии распределения и стабилизации дисперсии.
Бокса-Кокса преобразование: Используется для приведения данных к нормальному распределению, если они положительные.",,,
Ансамблирование моделей,Чем отличается бэггинг от бустинга?,"Бэггинг (bootstrap aggregating) заключается в параллельном обучении нескольких моделей на различных подвыборках данных с последующим усреднением их предсказаний для повышения стабильности и снижения дисперсии модели. Бустинг же обучает модели последовательно, каждая последующая модель исправляет ошибки предыдущей, что позволяет уменьшить смещение и повысить точность за счет усиления ""слабых"" моделей. Бэггинг обычно снижает дисперсию, тогда как бустинг фокусируется на уменьшении смещения.",,,"https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii#busting#:~:text=Отличие%20состоит%20в%20том,%20что"
Ансамблирование моделей,Ансамбль стекинга уменьшает смещение модели?,"Ансамбль стекинга (stacking) помогает уменьшить как смещение, так и дисперсию модели за счёт комбинации предсказаний нескольких разных моделей с помощью модели-метаучителя. Вместо последовательного исправления ошибок, как в бустинге, стекинг использует разнообразие моделей и данных для более точного усредненного предсказания. Снижение смещения возможно за счет выбора сильных базовых моделей и оптимальной мета-модели.",,Wildberries,https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii#busting#:~:text=С%20точки%20зрения%20смещения%20и%20разброса%20стекинг
Ансамблирование моделей,Что такое ансамбль моделей? Какие виды ансамблей существуют?,"Ансамбль моделей — это метод машинного обучения, в котором несколько моделей объединяются для улучшения точности и стабильности предсказаний. Существуют различные виды ансамблей, включая:

Бэггинг (Bootstrap Aggregating): Создание нескольких подвыборок из обучающего набора с возвращением и обучение отдельных моделей на этих подвыборках. Итоговый результат получается путем усреднения (для регрессии) или голосования (для классификации).
Бустинг: Последовательное обучение моделей, где каждая новая модель исправляет ошибки предыдущей. Примеры — AdaBoost, Gradient Boosting.
Стекинг (Stacking): Комбинирование предсказаний нескольких базовых моделей с использованием мета-модели, которая обучается на этих предсказаниях.",,,https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii
Ансамблирование моделей,Чем отличается блендинг от стэкинга?,"Блендинг и стекинг — это методы ансамблирования моделей, но они имеют некоторые различия:

Блендинг: Использует простую модель для объединения предсказаний нескольких базовых моделей. Обычно используются методы, такие как линейная регрессия, для взвешивания предсказаний.
Стекинг: Включает обучение мета-модели на предсказаниях базовых моделей. Мета-модель обучается предсказывать целевую переменную, используя предсказания других моделей в качестве входных данных. Стекинг позволяет использовать более сложные модели для комбинирования результатов.",,,https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii
Ансамблирование моделей,Что такое бутстрэп?,"Бутстрэп — это метод статистического анализа, который позволяет оценивать параметры модели или распределения данных через создание множества подвыборок с возвращением из исходного набора данных. Этот метод помогает оценивать устойчивость статистических оценок и обеспечивает способ оценки ошибок без предположений о форме распределения данных.
",Middle,bidease (с подводкой к ЦПТ),https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii#busting#:~:text=называется%20бутстрепом%20(bootstrap)
Байесовская инференция,"Чем отличаются частотные модели от Байесовских?
В чём плюсы и второй над первой?","Частотные модели:

Основной подход: Оценивают параметры модели, максимизируя правдоподобие (maximum likelihood estimation, MLE).
Гипотезы: Рассматривают параметры как фиксированные, но неизвестные величины.
Оценка неопределённости: Оценивается с помощью методов вроде доверительных интервалов и бутстрепа.
Байесовские модели:

Основной подход: Используют теорему Байеса для обновления распределения вероятностей на основе наблюдаемых данных.
Гипотезы: Рассматривают параметры как случайные величины с определённым априорным распределением.
Оценка неопределённости: Обеспечивают полное распределение вероятностей для параметров, что даёт более полную картину неопределённости.
Плюсы Байесовских моделей:

Априорные знания: Позволяют включать априорную информацию о параметрах, что может быть полезно при небольших объёмах данных.
Интерпретируемость: Предоставляют более полную картину неопределённости через апостериорные распределения.
Робастность: Часто более устойчивы к переобучению, особенно при наличии шума в данных.",Middle (R&D),bidease,
Глубокое обучение,"Инициализация весов-Как сделать, чтобы при каждом запуске кода модель обучаемая при помощи градиентного спуска сходилась к одной и той же точке?","Для достижения детерминированности обучения и схождения к одной и той же точке, необходимо:

Фиксировать случайный сид: Установить начальное значение для генератора случайных чисел, что обеспечит одинаковую инициализацию весов и порядок мини-батчей.

python
Копировать код
import numpy as np
import random
import torch

seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
Использовать одинаковые начальные условия: Инициализировать веса одинаковым образом при каждом запуске.

Убедиться в детерминированности операций: В некоторых библиотеках, таких как PyTorch, необходимо включить детерминированный режим для некоторых операций:

python
Копировать код
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False",,,
Глубокое обучение,Что такое нормализация в контексте нейросетей? Какая польза от использования нормализации в нейросетях?,"Нормализация – это процесс изменения распределения входных данных или промежуточных активаций в сети, чтобы сделать их более стабильными.

Польза от нормализации:

Ускорение обучения: Улучшает сходимость градиентного спуска.
Стабильность: Снижает риск ""взрывного"" или ""исчезающего"" градиента.
Улучшение генерализации: Снижает вероятность переобучения.
Наиболее известные техники нормализации в нейросетях включают Batch Normalization, Layer Normalization и других.",,Wildberries,
Глубокое обучение,"Если взять батч размером 10к, обучить на нём модель, потом
увеличить его размер до 20к, какие параметры нужно дотюнить,
как и почему?","При изменении размера батча может потребоваться:

Изменение скорости обучения: Больший размер батча может привести к более устойчивым обновлениям градиента, поэтому можно увеличить скорость обучения. Однако, при слишком большом размере батча скорость обучения может потребоваться уменьшить, чтобы избежать переполнения и повысить точность.

Регулировка числа эпох: С увеличением размера батча может потребоваться уменьшить число эпох, так как каждый шаг теперь будет включать больше данных, и модель будет быстрее проходить через весь датасет.

Регуляризация: Возможно, потребуется скорректировать параметры регуляризации (например, коэффициент dropout или L2-регуляризацию), так как эффект от регулирования может измениться с увеличением размера батча.",Middle,bidease,
Глубокое обучение,"Если при обучении использовался dropout для регуляризации,
что происходит на инференсе модели?","Dropout во время инференса отключается. На этапе инференса (т.е., при предсказании) используется вся сеть без случайного отключения нейронов, и веса масштабируются, чтобы учесть, что на этапе обучения некоторые нейроны отключались. Это обеспечивает стабильность и согласованность результатов.",,,https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout
Глубокое обучение,Техники регуляризации в глубоком обучении.,"Основные техники регуляризации:

Dropout: Случайное отключение нейронов во время обучения для предотвращения переобучения.
L1 и L2-регуляризация: Добавление штрафа к функции потерь за большие значения весов, что способствует их уменьшению.
Early Stopping: Остановка обучения при ухудшении качества на валидационной выборке.
Data Augmentation: Искусственное увеличение тренировочного набора данных путём применения трансформаций к данным.
Batch Normalization: Нормализация выходов слоёв для уменьшения внутреннего смещения и ускорения обучения.",Стажер,АИРИ,
Глубокое обучение,Как работает Dropout?,"Dropout – это техника, при которой в процессе обучения нейросети случайно ""выключаются"" (устанавливаются в ноль) некоторые нейроны с вероятностью 
𝑝
p. Это предотвращает коадаптацию нейронов и способствует более устойчивому обобщению сети. При этом веса на этапе инференса масштабируются на
1−p, чтобы учесть изменение структуры сети.",Стажер,АИРИ,
Глубокое обучение,Для чего делить значения на 1-p во время обучения в Dropout?,"Во время обучения веса делятся на 1−p для компенсации эффекта случайного отключения нейронов. Это делается, чтобы обеспечить правильный масштаб активностей при инференсе, где все нейроны активны. Это позволяет сети работать с той же интенсивностью сигналов, как и на этапе обучения.",Стажер,АИРИ,
Глубокое обучение,Как работает BatchNorm?,"Batch Normalization (BatchNorm) – это процесс нормализации активаций в нейронной сети по мини-батчам. На каждом слое и для каждого мини-батча:

Вычисляются среднее и дисперсия активаций.
Активации нормализуются с использованием этих значений.
Применяются обучаемые параметры масштабирования и сдвига, чтобы вернуть модель к выражательной способности.
BatchNorm ускоряет обучение и улучшает сходимость за счёт снижения внутренних изменений в распределении активаций.",Стажер,АИРИ,
Глубокое обучение,Для чего нужно накопление среднего и дисперсии в батчнорм во время обучения?,"Накопление среднего и дисперсии в BatchNorm нужно для того, чтобы иметь возможность нормализовать данные на этапе инференса, когда проход через сеть осуществляется с одним образцом (или без расчёта средних и дисперсий для мини-батча). Это позволяет использовать статистику, накопленную на этапе обучения, для предсказаний и делает инференс детерминированным.",Стажер,АИРИ,
Кластеризация,Кластеризация-Что это такое? Какие методы знаете?,"Кластеризация – это метод анализа данных, который объединяет данные в группы (кластеры) на основе схожести между элементами в каждом кластере.

Методы кластеризации:

K-средние (K-means): Разделяет данные на k кластеров, минимизируя внутрикластерное расхождение.
Иерархическая кластеризация: Создаёт древовидную структуру кластеров (дендрограмму), с объединением или разбиением кластеров.
DBSCAN: Кластеризует данные на основе плотности, определяя плотные регионы как кластеры.
Gaussian Mixture Models (GMM): Представляет данные как комбинацию нескольких гауссовых распределений.",,,
Кластеризация,Для чего может использоваться кластеризация?,"Кластеризация используется для разделения данных на группы (кластеры) на основе их сходства, что позволяет обнаружить скрытые структуры в данных без использования меток. Она может применяться для сегментации клиентов в маркетинге, анализа социального поведения, уменьшения размерности данных и выявления аномалий. Этот метод полезен в случаях, когда необходимо найти закономерности или тренды в больших объемах данных.",,,
Кластеризация,Объясните разницу между knn и k-means,"K-nearest neighbors (k-NN) — это алгоритм обучения с учителем, используемый для классификации и регрессии, который определяет класс или значение объекта на основе большинства его ближайших соседей в пространстве признаков. В отличие от него, k-means — это алгоритм обучения без учителя, применяемый для кластеризации, который делит данные на k кластеров, минимизируя внутрикластерное расстояние. Основное различие в том, что k-NN классифицирует новые данные на основе размеченного обучающего набора, тогда как k-means разделяет данные на кластеры без заранее известной разметки.",,,https://www.simplilearn.com/tutorials/machine-learning-tutorial/machine-learning-interview-questions#:~:text=Compare%20K%2Dmeans%20and%20KNN%20Algorithms
Кластеризация,Какие метрики для классификации и кластерации использовал?,"Метрики для классификации:
Accuracy (Точность): Доля правильно классифицированных объектов.
Precision (Точность положительных прогнозов): Доля истинных положительных среди всех объектов, предсказанных как положительные.
Recall (Полнота): Доля истинных положительных объектов среди всех положительных в выборке.
F1-score: Гармоническое среднее точности и полноты.
ROC-AUC: Площадь под ROC-кривой, показывающая соотношение истинных положительных и ложных положительных при различных порогах классификации.
Log Loss (Логарифмическая потеря): Оценка качества классификации, использующая вероятности предсказаний.
Метрики для кластеризации:
Silhouette Score (Коэффициент силуэта): Оценивает, насколько объект близок к объектам своего кластера по сравнению с объектами других кластеров.
Dunn Index (Индекс Данна): Отношение минимального межкластерного расстояния к максимальному внутрикластерному расстоянию.
Davies-Bouldin Index (Индекс Дэвиса-Болдина): Среднее отношение внутрикластерных дистанций к межкластерным.
Adjusted Rand Index (Корректированный индекс Рэнда): Сравнивает сходство между двумя наборами кластеров, учитывая случайное распределение.
Normalized Mutual Information (Нормированная взаимная информация): Измеряет информацию, общую для двух распределений, нормированную по информации в каждом.",,,
Кластеризация,Расскажи про иерархическую кластеризацию?,"Иерархическая кластеризация создает дерево кластеров (дендрограмму) и может быть агломеративной или дивизионной. В агломеративной кластеризации каждый объект сначала рассматривается как отдельный кластер, и кластеры объединяются снизу вверх, пока не останется один кластер. В дивизионной кластеризации процесс идет сверху вниз, начиная с одного кластера и разделяя его до получения отдельных объектов. Расстояния между кластерами могут рассчитываться различными методами, такими как метод одиночной связи, полной связи или средней связи.",,,
Кластеризация,Какова алгоритмическая сложность knn?,"Алгоритмическая сложность KNN (k-ближайших соседей) составляет O(n * d) для поиска ближайших соседей в простейшем случае, где n — количество объектов в тренировочном наборе, а d — размерность пространства. При использовании структур данных, таких как kd-деревья, сложность может уменьшиться до O(log n) для поиска, но построение дерева имеет сложность O(n * log n).",,,
Кластеризация,Как можно подобрать оптимальное число кластеров в KMeans?,"Оптимальное число кластеров в KMeans можно определить несколькими методами:

Метод локтя (Elbow Method): Строится график суммы квадратов внутрикластерных расстояний для разных значений k, и выбирается точка, где происходит замедление уменьшения этой суммы.

Коэффициент силуэта (Silhouette Score): Оценивает плотность и разделение кластеров; выбирается значение k, при котором этот коэффициент максимален.

Критерий Гапа (Gap Statistic): Сравнивает логарифм внутрикластерной дисперсии с ожидаемой дисперсией, полученной на случайных данных.

Сравнение с реальными данными: Если доступны истинные метки, можно сравнить результаты кластеризации с ними, используя метрики, такие как Adjusted Rand Index.",Junior NLP,Сбер,
Линейная регрессия,Линейная регрессия-Описать алгоритм работы,"Линейная регрессия — это метод моделирования зависимости между зависимой переменной (откликом) и одной или несколькими независимыми переменными (факторами). Алгоритм ищет линейное уравнение, которое минимизирует разницу между предсказанными и реальными значениями. Обучение заключается в нахождении коэффициентов, которые минимизируют функцию потерь, обычно среднеквадратичную ошибку.",,,
Линейная регрессия,Плюсы/минусы линейной регрессии,"Плюсы:

Простота и интерпретируемость.
Быстрота обучения на больших объемах данных.
Легкость в обработке мультиколлинеарных данных.
Минусы:

Чувствительность к выбросам.
Предположение линейности между переменными, что может не всегда выполняться.
Не подходит для моделирования сложных нелинейных зависимостей.",,,
Линейная регрессия,Линейная регрессия-Функция потерь,"Функция потерь в линейной регрессии — это среднеквадратичная ошибка (MSE). Она рассчитывает среднее значение квадратов разности между реальными значениями и предсказанными значениями. MSE стремится минимизировать эту разность, подбирая оптимальные коэффициенты регрессии.",,,
Линейная регрессия,Чем плохи большие веса?,"Большие веса в модели могут привести к переобучению, так как модель может слишком сильно подстроиться под тренировочные данные, теряя способность к генерализации на новых данных. Это может также привести к высокой вариативности в предсказаниях и увеличению чувствительности к шуму в данных.",,,
Логистическая регрессия,Логистическая регрессия-Описать алгоритм работы,"Логистическая регрессия используется для решения задач классификации, моделируя вероятность принадлежности объекта к определенному классу. Она использует логистическую функцию для преобразования линейной комбинации входных признаков в вероятность, ограниченную диапазоном от 0 до 1. Алгоритм находит коэффициенты, которые максимизируют логарифмическую правдоподобие (Log-Likelihood) функции.",,,
Логистическая регрессия,Логистическая регрессия - Какая функция потерь?,"Функция потерь в логистической регрессии — это логарифмическая функция правдоподобия (Log-Loss) или бинарная кросс-энтропия. Она измеряет расхождение между предсказанными вероятностями и реальными метками классов, стремясь минимизировать это расхождение.",,,
Математика (Оптимизация),"Что такое порядок градиента в теории оптимизации?
Что такое 0ой порядок, где и как используется? Почему бы не 
использовать матрицу Хесса на практике для расчёта 
градиента, ведь он точнее и стабильнее (отсылка к хранению 
градиентов в памяти)?","Порядок градиента в теории оптимизации указывает на использование производных функций для обновления параметров. 0-й порядок использует только значения функции, 1-й порядок использует первые производные (градиенты), а 2-й порядок использует вторые производные (матрицы Гессе). Методы нулевого порядка, такие как метод случайного поиска или метод безградиентной оптимизации, используют только значения функции для обновления параметров. Они применяются, когда вычисление градиентов невозможно или дорого, например, при оптимизации черного ящика или функций с дискретными значениями. Матрица Гессе, учитывающая вторые производные, может дать более точные результаты, но её использование связано с высокой вычислительной сложностью и большим потреблением памяти, особенно в задачах с большим количеством параметров. Это делает её применение неэффективным для многих практических задач. Методы первого порядка, такие как градиентный спуск, проще и более масштабируемы, поэтому они чаще используются на практике.",Middle (R&D),bidease,
Метод опорных векторов / SVM,SVM-Расскажите суть SVM метода,"Метод опорных векторов (Support Vector Machine, SVM) — это алгоритм машинного обучения для задач классификации и регрессии, который находит гиперплоскость, максимально разделяющую данные на классы. Цель SVM — максимизировать зазор между данными разных классов, используя точки, называемые опорными векторами. SVM также может использовать ядра (kernel functions) для классификации нелинейных данных, трансформируя пространство признаков.",,,
Методы регуляризации,Чем L1 от L2 отличается?,"L1 и L2 — это виды регуляризации, используемые для предотвращения переобучения модели.

L1-регуляризация (Lasso): Использует сумму абсолютных значений весов. Она может обнулять некоторые веса, тем самым уменьшая количество признаков.
L2-регуляризация (Ridge): Использует сумму квадратов весов. Она уменьшает величину весов, но редко приводит их к нулю.",,,
Методы регуляризации,Как именно работают L1 и L2 методы регуляризации?,"L1-регуляризация (Lasso) добавляет штраф к функции потерь, пропорциональный сумме абсолютных значений коэффициентов. Это приводит к обнулению некоторых весов, отбрасывая несущественные признаки.
L2-регуляризация (Ridge) добавляет штраф, пропорциональный сумме квадратов коэффициентов, сглаживая весовые коэффициенты и предотвращая их чрезмерное увеличение.",,,
Методы регуляризации,Каким образом L1 обнуляет веса?,"L1-регуляризация накладывает линейное ограничение на веса, что приводит к тому, что при минимизации функции потерь некоторые веса стремятся к нулю. Это создает разреженные модели, где многие веса обнуляются, отбрасывая несущественные признаки и повышая интерпретируемость модели.",,,
Метрики и функции потерь,Какие метрики регрессии существуют?,"Среднеквадратичная ошибка (MSE): Среднее квадратов ошибок между предсказанными и истинными значениями.
Средняя абсолютная ошибка (MAE): Среднее абсолютных ошибок между предсказанными и истинными значениями.
Коэффициент детерминации (R²): Оценивает, насколько хорошо модель объясняет вариативность данных.
Средняя абсолютная процентная ошибка (MAPE): Среднее абсолютных процентных ошибок между предсказанными и истинными значениями.
Среднеквадратичный логарифмический корень ошибки (RMSLE): Корень среднего квадрата логарифмических ошибок.",,,https://loginom.ru/blog/quality-metrics#:~:text=Он%20включает%20в%20себя%20следующие%20меры%3A
Метрики и функции потерь,Кросс-энтропия - это ..?,"Кросс-энтропия — это функция потерь, измеряющая расхождение между истинным распределением классов и распределением, предсказанным моделью. В задачах классификации кросс-энтропия оценивает, насколько вероятности, предсказанные моделью, совпадают с истинными метками классов.",,,
Метрики и функции потерь,Кросс-энтропия: какое основание у логарифма и важно ли оно,"Основание логарифма в кросс-энтропии обычно не важно, так как это просто масштабирует значение потерь. Однако на практике чаще всего используется натуральный логарифм (основание e), так как это приводит к удобной аналитической форме для вычислений и оптимизации.",,,https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii#entropiya-i-divergencziya-kulbaka-lejblera:~:text=В%20классическом%20определении%20логарифм
Метрики и функции потерь,Назовите 5 функций потерь,"Среднеквадратичная ошибка (MSE)
Средняя абсолютная ошибка (MAE)
Логистическая потеря (Log Loss)
Кросс-энтропия
Hinge Loss (для SVM)",,,
Метрики и функции потерь,Какие функции потерь для классификации?,"Логистическая потеря (Log Loss)
Кросс-энтропия
Hinge Loss
Квадратная потеря (в редких случаях, когда классы числовые)
Focal Loss (для борьбы с несбалансированными данными)",,,
Метрики и функции потерь,Какие функции потерь для регрессии?,"Среднеквадратичная ошибка (MSE)
Средняя абсолютная ошибка (MAE)
Хьюберовская потеря (Huber Loss)
Среднеквадратичный логарифмический корень ошибки (RMSLE)
Квинтильная потеря (Quantile Loss)",,,
Метрики и функции потерь,Почему MSE нельзя брать в задаче классификации?,"MSE нельзя использовать в задачах классификации, так как она не подходит для категориальных данных и плохо справляется с моделированием вероятностей. MSE может создать неоднозначные решения, когда модель пытается минимизировать ошибки, не принимая во внимание вероятности классов, что приводит к неустойчивым и неточным предсказаниям.",,,https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/
Метрики и функции потерь,"Чем отличается MAE от MAPE, что более понятно бизнесу?","MAE (средняя абсолютная ошибка) измеряет среднюю абсолютную разницу между предсказанными и реальными значениями, что полезно, когда важна конкретная величина ошибки. MAPE (средняя абсолютная процентная ошибка) измеряет среднюю процентную разницу и более понятна бизнесу, так как представляет ошибки в виде процентов, что позволяет легко интерпретировать ошибки в контексте масштабов данных.",,,
Метрики и функции потерь,Что такое MAPE?,"MAPE (средняя абсолютная процентная ошибка) — это метрика регрессии, измеряющая среднюю процентную ошибку между предсказанными и реальными значениями. MAPE вычисляет средний абсолютный процент разницы, что делает её полезной для оценки относительных ошибок в бизнес-контексте.",,,
Метрики и функции потерь,"Какая модель классификации из трёх будет иметь более высокую предсказательную способность, если метрика ROC-AUC = (0.51, 0.65, 0.88) соответственно для каждой модели?","Модель с ROC-AUC = 0.88 будет иметь более высокую предсказательную способность. ROC-AUC измеряет способность модели различать классы; чем выше значение, тем лучше модель. Значение 0.88 указывает на хорошую дискриминацию, тогда как 0.51 близко к случайному угадыванию.",,"Wildberries, Сбер",
Метрики и функции потерь,Что произойдёт с графиком ROC-AUC если мы возведём все предсказания в квадрат?,"Возведение предсказаний в квадрат не изменит график ROC-AUC и значение метрики, поскольку ROC-AUC зависит только от порядка предсказаний, а не от их абсолютных значений. При возведении в квадрат порядок вероятностей не изменится, поэтому ROC-кривая и AUC останутся прежними.",,"Wildberries, Сбер",
Метрики и функции потерь,Что будет с графиком ROC-AUC и метрикой если к предикту добавить константу?,"Добавление константы к предсказаниям также не повлияет на график ROC-AUC и значение метрики, поскольку ROC-AUC оценивает только порядок вероятностей. Порядок останется неизменным, так как добавление константы ко всем значениям не меняет их относительное расположение.",,"Wildberries, Сбер",
Метрики и функции потерь,Расскажите про ROC-AUC. Что означает значение 0.5 ROC-AUC?,"ROC-AUC (Receiver Operating Characteristic - Area Under Curve) — это метрика, оценивающая качество классификатора по способности различать классы. ROC-кривая строится по оси истинных положительных значений (TPR) и ложных положительных значений (FPR) для различных порогов классификации.

Значение ROC-AUC = 0.5 означает, что модель не различает классы лучше случайного угадывания. Это указывает на то, что модель не имеет дискриминационной способности.

Значение ROC-AUC > 0.5 указывает, что модель имеет некоторую способность различать классы.

Значение ROC-AUC = 1.0 указывает на идеальную классификацию, где все объекты классов правильно разделены.",,"Wildberries, Сбер",
Метрики и функции потерь,"Что произойдет с ROC-AUC, если мы разделим все вероятности на 10? А если умножим?","Разделение всех вероятностей на 10 или их умножение не изменит график ROC-AUC и значение метрики, так как они зависят только от порядка предсказаний. Масштабирование вероятностей не изменяет их относительное расположение, поэтому ROC-кривая и AUC останутся прежними.",,"Wildberries, Сбер",
Метрики и функции потерь,Можно ли в задаче классификации оптимизировать метрику accuracy градиентным спуском?,"Нет, оптимизация метрики accuracy напрямую градиентным спуском невозможна, потому что accuracy — это недифференцируемая функция, основанная на пороговом значении. Вместо этого используют дифференцируемые функции потерь, такие как кросс-энтропия, чтобы обучить модель и затем оценивать accuracy на валидационной выборке.",,Wildberries,
Обработка данных,"Подготовка обучающей выборки-Что делать, если в данных есть дисбаланс классов?","При дисбалансе классов можно использовать несколько подходов:

Взвешивание классов: Присвоение большего веса менее представленным классам при обучении модели.

Undersampling: Уменьшение размера более многочисленного класса для выравнивания частот классов.

Oversampling: Увеличение размера менее многочисленного класса путем повторения или генерации новых данных (например, SMOTE).

Использование специализированных алгоритмов: Например, применение алгоритмов, таких как Random Forest с балансировкой или адаптацией, для работы с несбалансированными данными.

Создание меток: Преобразование задачи в многоклассовую классификацию с учётом баланса.",,,
Обработка данных,Выбросы-Какие методы поиска выбросов вы знаете?,"Метод сигм (Z-score): Оценка того, насколько далеко значение отклоняется от среднего (в количествах сигм).

Метод межквартильного диапазона (IQR): Выбросы определяются как значения, выходящие за границы 
q1 - 1,5 * IQR и Q3 + 1,5 * IQR

Визуальные методы: Использование графиков, таких как диаграммы размаха (boxplots) или гистограммы.

Кластеризация: Выбросы могут быть определены как точки, не принадлежащие ни одному кластеру.

Методы обучения: Использование алгоритмов, таких как Isolation Forest или One-Class SVM, для выявления выбросов.

Метод Локальных Отметок (LOF): Оценка выбросов по плотности точек относительно соседей.",,,
Отбор признаков / Feature engineering,Что делать если много признаков?,"Отбор признаков: Использование методов для выбора наиболее значимых признаков, таких как RFE или Lasso.

Сокращение размерности: Применение методов, таких как PCA или t-SNE, для уменьшения размерности пространства признаков.

Удаление коррелированных признаков: Исключение признаков, имеющих высокую корреляцию, чтобы уменьшить избыточность.

Регуляризация: Применение L1 или L2 регуляризации для снижения влияния незначительных признаков.

Feature Engineering: Создание новых информативных признаков и удаление ненужных.",,,
Отбор признаков / Feature engineering,Какие методы отбора признаков знаешь?,"Filter Methods (Методы фильтрации): Используют статистические тесты для оценки значимости признаков (например, метод выбора на основе взаимной информации).

Wrapper Methods (Методы оболочки): Исследуют различные подмножества признаков, обучая модель и оценивая её производительность (например, рекурсивный отбор признаков - RFE).

Embedded Methods (Встроенные методы): Встроены в модели, такие как Lasso или Random Forest, и автоматически отбирают важные признаки в процессе обучения.

Методы жадного поиска (Greedy Search Methods): Forward Selection и Backward Elimination для последовательного добавления или удаления признаков.

Permutation Importance: Оценивает значимость признаков на основе влияния на качество модели после случайного перемешивания значений признаков.",,,
Отбор признаков / Feature engineering,Что такое permutations importance?,"Permutation Importance — это метод оценки важности признаков, основанный на измерении изменения производительности модели после случайного перемешивания значений конкретного признака. Если перемешивание признака сильно ухудшает качество модели, то этот признак важен. Метод особенно полезен для интерпретации моделей и определения значимости признаков.",,,
Отбор признаков / Feature engineering,Какие методы сокращения размерности знаешь?,"PCA (Анализ главных компонент): Преобразует данные в новое пространство меньшей размерности с минимальной потерей информации.

LDA (Анализ линейных дискриминантов): Максимизирует разницу между классами, используя линейные комбинации признаков.

t-SNE: Метод для визуализации высокоразмерных данных в пространстве низкой размерности, особенно полезен для кластеризации.

UMAP: Подобен t-SNE, но более эффективен и устойчив для больших наборов данных.

Autoencoders: Нейронные сети, обучающиеся кодировать данные в пространство меньшей размерности и декодировать обратно с минимальной потерей информации.",,,
Отбор признаков / Feature engineering,Что такое PCA,"PCA (Principal Component Analysis, Анализ главных компонент) — это метод сокращения размерности, который преобразует данные в новое пространство, где новые оси (главные компоненты) представляют собой линейные комбинации исходных признаков. Эти компоненты упорядочены по доле объясняемой дисперсии данных, что позволяет сократить размерность, сохраняя максимальную возможную информацию. PCA используется для визуализации и предварительной обработки данных.",,,
Отбор признаков / Feature engineering,"Обработка категориальных данных-One-hot-encoder, label encoder, helmert, Frequency","One-hot encoding: Преобразует категориальные данные в бинарные векторы, где каждая уникальная категория кодируется отдельной колонкой с 0 и 1.

Label encoding: Преобразует категории в числовые значения, присваивая каждой уникальной категории уникальное целое число. Этот метод может быть менее подходящим для неупорядоченных категорий.

Helmert coding: Кодирование, которое сравнивает среднее каждой категории с средним всех предыдущих категорий.

Frequency encoding: Присваивает каждой категории значение частоты её появления в данных. Это может быть полезно для категорий с различной частотой появления.",,,https://youtu.be/589nCGeWG1w?si=lum5UF-IUFOw1AV7
Отбор признаков / Feature engineering,Категориальные признаки-Расскажите как преобразовываете категориальные переменные?,"One-hot encoding: Преобразую категориальные переменные в бинарные векторы, используя pandas или scikit-learn. Этот метод полезен для признаков с небольшим числом уникальных значений.

Label encoding: Использую label encoding для упорядоченных категориальных переменных, где существует естественный порядок между категориями.

Target encoding: Среднее значение целевой переменной для каждой категории, подходит для задач классификации с категориальными признаками, которые сильно коррелируют с целевой переменной.

Frequency encoding: Использую частоту появления категории в данных, если число категорий велико и они имеют различную частоту появления.

Embedding: В задачах с большими категориями и использованием нейронных сетей применяю векторные представления категорий, которые обучаются вместе с моделью.

Hashing: Использую хэш-функции для преобразования категориальных признаков в фиксированное количество столбцов, чтобы уменьшить размерность пространства.

Backward difference coding: Сравниваю среднее каждой категории со средним всех последующих категорий.",,,
Постановка задач,Классификация-Дайте определение классификации. Какие методы классификации вы знаете?,"Классификация — это задача машинного обучения, направленная на определение принадлежности объекта к одному из заранее определённых классов на основе его характеристик (признаков). Методы классификации включают:

Логистическая регрессия: Статистический метод для бинарной классификации.
Метод опорных векторов (SVM): Строит гиперплоскость, разделяющую классы с максимальным зазором.
Наивный байесовский классификатор: Использует теорему Байеса для классификации на основе вероятностей.
k-ближайших соседей (k-NN): Классифицирует объект на основе класса ближайших соседей в пространстве признаков.
Деревья решений: Построение дерева на основе условий, делящих данные на классы.
Случайный лес (Random Forest): Комбинация нескольких деревьев решений для улучшения стабильности и точности.
Градиентный бустинг: Итеративное обучение модели на антиградиентах ошибки предыдущих моделей.
Нейронные сети: Использование слоёв нейронов для обучения сложным зависимостям между данными и классами.",,,
Постановка задач,Что включает в себя задача прогнозирования?,"Задача прогнозирования включает в себя анализ исторических данных для построения модели, которая может предсказать будущие значения целевой переменной. Основные этапы:

Сбор данных: Получение и подготовка данных для анализа.
Предварительная обработка данных: Очистка данных, заполнение пропусков, кодирование категориальных переменных.
Выбор модели: Определение подходящей модели для задачи (например, регрессия, временные ряды).
Обучение модели: Настройка параметров модели на тренировочных данных.
Валидация модели: Оценка модели на отложенной выборке для проверки точности и генерализации.
Тестирование и внедрение: Оценка производительности модели и её применение для реальных прогнозов.
Мониторинг: Постоянное отслеживание модели для своевременного обновления и улучшения.",,,
"Процесс обучения, недо- и переобучение",Градиентный спуск-Что такое градиент?,"Градиент — это вектор, указывающий направление наибольшего увеличения функции в заданной точке. В контексте оптимизации градиент используется для обновления параметров модели в направлении, противоположном градиенту, с целью минимизации функции потерь. Градиентный спуск обновляет параметры модели, чтобы минимизировать ошибку.",,,https://teletype.in/@python_academy/J5J5mioH7
"Процесс обучения, недо- и переобучение","Если взять GD и SGD и есть возможность использования GD, что лучше сработает?","Выбор между Градиентным спуском (GD) и Стохастическим градиентным спуском (SGD) зависит от задачи:

GD использует весь набор данных для вычисления градиента и выполняет одно обновление. Он более точен, но медленен и требует много памяти для больших наборов данных.

SGD обновляет параметры на основе одного или нескольких случайно выбранных примеров из набора данных, что делает его более быстрым и эффективным для больших наборов данных. Однако он может быть менее стабильным из-за стохастического характера обновлений.

Мини-батч градиентный спуск является компромиссом, совмещая преимущества GD и SGD, и часто предпочтителен для практического использования.",,,
"Процесс обучения, недо- и переобучение",Борьба с переобучением-Какие варианты есть?,"Регуляризация: Применение L1 или L2 регуляризации для уменьшения избыточных параметров модели.

Dropout: Случайное отключение нейронов в процессе обучения для предотвращения коадаптации.

Аугментация данных: Искусственное увеличение размера обучающего набора данных с помощью различных трансформаций.

Раннее прекращение (Early Stopping): Остановка обучения, когда ошибка на валидационной выборке начинает увеличиваться.

Сокращение размерности: Удаление нерелевантных или коррелированных признаков из данных.

Сбор большего объема данных: Добавление новых примеров для улучшения способности модели обобщать данные.

Bagging и Boosting: Методы ансамблирования, такие как случайный лес и градиентный бустинг, для уменьшения переобучения за счёт комбинации нескольких моделей.",,Яндекс,
"Процесс обучения, недо- и переобучение",Что такое компромис bias & variance (смещение и разброс),"Компромисс смещения и разброса (bias-variance tradeoff) — это баланс между двумя основными видами ошибок, возникающими при обучении модели:

Смещение (Bias): Ошибка, возникающая из-за слишком упрощенной модели, неспособной учесть сложные зависимости в данных (недообучение).

Разброс (Variance): Ошибка, возникающая из-за слишком сложной модели, чрезмерно приспособившейся к обучающим данным (переобучение).

Цель — найти модель, которая минимизирует обе ошибки, достигая хорошей генерализации.",,Wildberries,https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition
"Процесс обучения, недо- и переобучение",Какой bias/variance у константной модели?,"У константной модели высокий bias и низкий variance. Такая модель игнорирует информацию о признаках и всегда предсказывает одно и то же значение (например, среднее по обучающему набору), что приводит к большим систематическим ошибкам (высокое смещение), но низкой вариативности в предсказаниях (низкий разброс).",,Wildberries,
"Процесс обучения, недо- и переобучение",В чем суть RLHF?,"RLHF (Reinforcement Learning from Human Feedback) — это подход, который использует методы обучения с подкреплением, чтобы улучшить качество моделей машинного обучения, используя обратную связь от человека. Цель — обучить модели, оптимизируя их поведение в соответствии с человеческими предпочтениями и оценками. Это делается через алгоритмы, которые учатся на данных о предпочтениях, предоставляемых экспертами или пользователями.",Стажер,АИРИ,
Решающие деревья и их ансамбли (лес и бустинг),Как работает градиентный бустинг на деревьях?,"Градиентный бустинг на деревьях — это метод ансамблирования, который строит последовательность моделей (обычно деревьев решений), где каждая последующая модель исправляет ошибки предыдущих. Основные этапы:

Инициализация: Начальная модель предсказывает среднее целевой переменной.

Итерации: На каждой итерации строится новое дерево, которое обучается на антиградиенте ошибки предыдущей модели. Антиградиент показывает направление, в котором нужно улучшить модель.

Обновление: Новая модель добавляется к ансамблю, суммируя её предсказания с предсказаниями предыдущих моделей.

Повторение: Процесс повторяется до достижения заданного числа итераций или сходимости.

Градиентный бустинг эффективно исправляет ошибки предыдущих моделей, улучшая точность и обобщающую способность.",,,
Решающие деревья и их ансамбли (лес и бустинг),Чем отличается градиентный бустинг от случайного леса?,"Структура:

Градиентный бустинг строит модели последовательно, каждая модель исправляет ошибки предыдущих.
Случайный лес строит модели (деревья решений) параллельно и объединяет их предсказания голосованием или усреднением.
Цель:

Градиентный бустинг минимизирует ошибку за счёт обучения на антиградиенте ошибки предыдущих моделей.
Случайный лес снижает разброс за счёт комбинации предсказаний нескольких деревьев.
Скорость обучения:

Градиентный бустинг медленнее обучается, так как модели строятся последовательно.
Случайный лес быстрее в обучении благодаря параллельности.
Чувствительность к переобучению:

Градиентный бустинг более чувствителен к переобучению из-за последовательного обучения.
Случайный лес более устойчив к переобучению благодаря случайности в выборе признаков и данных.",,,
Решающие деревья и их ансамбли (лес и бустинг),В чем смысл обучать на анти-градиенте?,"Обучение на анти-градиенте позволяет градиентному бустингу корректировать ошибки предыдущих моделей. Антиградиент ошибки текущей модели указывает направление, в котором необходимо обновить предсказания, чтобы минимизировать функцию потерь. Это позволяет каждой новой модели в ансамбле сосредоточиться на трудных для классификации примерах, улучшая точность и обобщение модели на обучающих данных.",,,
Решающие деревья и их ансамбли (лес и бустинг),Преимущества и недостатки градиентного бустинга,"Преимущества:

Высокая точность: Градиентный бустинг может достичь высокой точности, так как исправляет ошибки предыдущих моделей.
Гибкость: Поддерживает различные функции потерь и может применяться как для задач классификации, так и регрессии.
Обработка разнородных данных: Эффективен при работе с различными типами данных и не требует нормализации признаков.
Интерпретируемость: Возможность оценки важности признаков и интерпретации модели через анализ деревьев.
Недостатки:

Чувствительность к переобучению: Высокий риск переобучения, особенно при использовании слишком большого числа деревьев.
Высокая вычислительная сложность: Процесс обучения медленный и требует значительных вычислительных ресурсов.
Необходимость тщательной настройки: Требует оптимизации множества гиперпараметров, таких как количество деревьев, скорость обучения и глубина деревьев.
Чувствительность к выбросам: Может быть подвержен влиянию выбросов в данных, что влияет на качество модели.",,,
Решающие деревья и их ансамбли (лес и бустинг),"Почему бы вместо деревьев в градиентном бустинге не использовать, например, линейную регрессию?","Деревья решений в градиентном бустинге позволяют модели обучаться нелинейным зависимостям в данных, в то время как линейная регрессия ограничивается линейными зависимостями. Градиентный бустинг с деревьями может автоматически обрабатывать взаимодействия между признаками и сложные структуры данных, что делает его более универсальным и мощным для широкого круга задач. Линейная регрессия может быть менее эффективной в задачах, где присутствуют нелинейности и сложные зависимости.",,,
Решающие деревья и их ансамбли (лес и бустинг),Как именно получается итоговый ответ в градиентном бустинге?,"В градиентном бустинге итоговый ответ формируется как сумма предсказаний всех деревьев. Каждый последующий дерево обучается на ошибках предыдущих деревьев (антиградиентах функции потерь) и добавляется с определённым весом, зависящим от скорости обучения. Итоговое предсказание является взвешенной суммой всех деревьев, что позволяет модели исправлять ошибки и улучшать точность на каждом шаге.",,,
Решающие деревья и их ансамбли (лес и бустинг),Может ли градиентый бустинг переобучиться с увеличением количества деревьев?,"Да, градиентный бустинг может переобучиться с увеличением количества деревьев. Если модель имеет слишком много деревьев или слишком большие деревья, она может слишком сильно приспособиться к обучающим данным, что приводит к снижению обобщающей способности на новых данных. Это можно контролировать с помощью регуляризации и раннего прекращения обучения.",,,
Решающие деревья и их ансамбли (лес и бустинг),Какой глубины деревья используются в градиентном бустинге? Почему?,"В градиентном бустинге обычно используются деревья небольшой глубины, часто от 3 до 8 уровней. Небольшая глубина помогает избежать переобучения, поскольку более мелкие деревья менее подвержены слишком сильному приспособлению к данным. Они действуют как слабые модели, которые, в совокупности, создают мощную и обобщающую модель.",,,
Решающие деревья и их ансамбли (лес и бустинг),"Как изменится метрика ошибки, например MAE, если из бустинга убрать первое дерево. А если последнее?","Убрать первое дерево: MAE может увеличиться, поскольку первое дерево обычно задает начальную оценку целевой переменной и важный базовый уровень. Удаление может привести к более низкому качеству последующих исправлений.

Убрать последнее дерево: MAE, вероятно, уменьшится меньше, поскольку последнее дерево делает мелкие коррекции. Оно добавляет уточнения, но отсутствие одного дерева не так сильно повлияет на общую точность, как удаление первого дерева.",,,
Решающие деревья и их ансамбли (лес и бустинг),"Когда бустинг менее эффективен, чем линейная регрессия?","Бустинг может быть менее эффективен, чем линейная регрессия, когда:

Линейные данные: Если данные имеют сильную линейную зависимость, линейная регрессия может дать лучшие результаты с меньшими затратами.
Маленькие наборы данных: Для небольших наборов данных бустинг может быть избыточен и привести к переобучению.
Высокая вычислительная сложность: Линейная регрессия требует меньше времени и ресурсов на обучение.
Интерпретация модели: Линейная регрессия обеспечивает более простую интерпретацию коэффициентов модели, что может быть важным в некоторых бизнес-контекстах.",,,
Решающие деревья и их ансамбли (лес и бустинг),Как строится дерево решений,"Дерево решений строится путём рекурсивного разбиения данных на подмножества, основываясь на критерии информативности, таком как энтропия, индекс Джини или MSE. На каждом узле выбирается наилучший признак и порог для разделения данных, чтобы максимизировать разделяющую способность на классы или минимизировать ошибку. Этот процесс продолжается до достижения заданной глубины, минимума выборок в узле, или других условий остановки.",,,
Решающие деревья и их ансамбли (лес и бустинг),Может ли дерево решений показывать вероятность?,"Да, дерево решений может показывать вероятность классов. В каждом листе дерева можно вычислить вероятность каждого класса как долю объектов этого класса среди всех объектов в листе. Это позволяет дереву возвращать не только конкретный класс, но и вероятности принадлежности к каждому классу.",,,
Решающие деревья и их ансамбли (лес и бустинг),Как получается ответ целевой переменной в дереве решений,"Ответ целевой переменной в дереве решений получается путём прохождения по дереву от корня до листа, применяя условия на каждом узле. Объект сравнивается с условиями на узлах, и в зависимости от результата выбирается одна из ветвей. Когда объект достигает листа, предсказание определяется значением или распределением классов в этом листе.",,,
Решающие деревья и их ансамбли (лес и бустинг),"Что такое энтропия, дерево ее старается минимизировать или максимизировать при построении?","Энтропия — это мера неопределённости или хаоса в данных, часто используемая для оценки качества разделений в деревьях решений. При построении дерева решений энтропия стремится к минимизации, чтобы каждое разделение уменьшало неопределённость в данных и обеспечивало более чистые подмножества.",,,
Решающие деревья и их ансамбли (лес и бустинг),Какая метрика оптимизируется в регрессионном дереве при выборе разбиения для нового узла?,"В регрессионном дереве оптимизируется среднеквадратичная ошибка (MSE). Разбиения выбираются так, чтобы минимизировать сумму квадратов отклонений предсказанных значений от реальных в каждом подмножестве.",,,
Решающие деревья и их ансамбли (лес и бустинг),За что отвечает L2 регуляризация в дереве?,"L2 регуляризация в деревьях решений помогает предотвратить переобучение, ограничивая величину весов. Она добавляет штраф за большие веса, сглаживая распределение параметров и уменьшая вероятность чрезмерного приспособления модели к обучающим данным. Это полезно для повышения обобщающей способности деревьев.",,,
Решающие деревья и их ансамбли (лес и бустинг),"До какой степени строятся дерево, т.е критерий останова дерева?","Дерево решений строится до достижения одного или нескольких критериев останова, таких как:

Максимальная глубина: Ограничение глубины дерева, чтобы избежать слишком глубоких разбиений.
Минимальное количество выборок в листе: Дерево не разделяется, если количество объектов в узле меньше заданного порога.
Минимальное уменьшение ошибки: Разделение прекращается, если улучшение в метрике (например, энтропии или Gini) меньше заданного значения.
Максимальное число листьев: Ограничение числа конечных узлов (листьев) в дереве.",,,
Решающие деревья и их ансамбли (лес и бустинг),"На этапе, когда мы ищем наиболее информативное разделение, можем ли мы применить к этому градиентный бустинг? Почему?","Градиентный бустинг нельзя напрямую применить к этапу выбора разделений в дереве, поскольку он предназначен для улучшения предсказаний всей модели, а не для оптимизации отдельных разбиений. Выбор наиболее информативных разбиений в деревьях производится локально на основе метрик, таких как энтропия или Gini. Градиентный бустинг работает на уровне ансамбля, добавляя новые деревья, которые исправляют ошибки предыдущих.",,,
Решающие деревья и их ансамбли (лес и бустинг),Можем ли мы преобразовать дерево в подобие линейной функции и вычислить дерево градиентным бустингом? Можем ли мы применить антиградиент к дереву?,"Мы не можем преобразовать дерево решений в линейную функцию, так как деревья работают с нелинейными зависимостями. Однако градиентный бустинг использует деревья для моделирования антиградиентов, чтобы исправить ошибки предыдущих моделей. Градиентный бустинг применяет антиградиенты на уровне ансамбля, добавляя деревья, которые минимизируют функцию потерь. Антиградиенты определяют направление и величину, на которые каждое новое дерево корректирует модель, чтобы улучшить её производительность.",,,
Решающие деревья и их ансамбли (лес и бустинг),Что такое слуайный лес и как он строится?,"Случайный лес — это ансамблевый метод машинного обучения, состоящий из множества деревьев решений, обученных на разных подвыборках данных. Основные этапы построения:

Бэггинг: Выбор случайных подвыборок из обучающего набора с возвращением.
Строительство деревьев: Каждое дерево строится на отдельной подвыборке и использует случайное подмножество признаков для разбиений на каждом узле.
Комбинация результатов: Предсказания всех деревьев усредняются (для регрессии) или принимается большинство голосов (для классификации) для получения итогового результата.
Случайный лес снижает разброс за счёт ансамблирования, повышая устойчивость модели.",,,
Решающие деревья и их ансамбли (лес и бустинг),Какие преимущества и недостатки случайного леса?,"Преимущества:

Высокая точность: Часто превосходит одиночные модели за счёт объединения предсказаний нескольких деревьев.
Устойчивость к переобучению: Благодаря случайности в обучении деревьев.
Обработка разнородных данных: Эффективно справляется с различными типами данных и не требует нормализации.
Оценка важности признаков: Предоставляет меры значимости для каждого признака.
Недостатки:

Высокие вычислительные затраты: Обучение большого числа деревьев может быть медленным.
Сложность интерпретации: Менее интерпретируем, чем одиночные деревья решений.
Может страдать от избыточности: Если число деревьев слишком велико, производительность может улучшаться незначительно, в то время как вычислительные затраты продолжают расти.",,,
Решающие деревья и их ансамбли (лес и бустинг),Какой глубины деревья используются в случайном лесу?,"В случайном лесу обычно используются деревья большой глубины, вплоть до их полного разрастания (максимальная глубина, когда каждый узел до конца разделён). Это позволяет каждому дереву обучиться максимально полно на своей подвыборке данных, так как случайный лес уменьшает переобучение за счёт ансамблирования множества глубоких, разнообразных деревьев.",,,
Решающие деревья и их ансамбли (лес и бустинг),Что может вызвать переобучение - ещё одно дерево в случайном лесу или в бустинге?,"Ещё одно дерево в бустинге может вызвать переобучение, так как бустинг добавляет деревья последовательно, и каждое дерево исправляет ошибки предыдущих. Если добавить слишком много деревьев, модель может слишком сильно приспособиться к обучающим данным. В случайном лесе, благодаря параллельной природе и бэггингу, добавление ещё одного дерева имеет меньше шансов вызвать переобучение.",,,
Решающие деревья и их ансамбли (лес и бустинг),"Вы обучили лес на выборке, целевые переменные которой состоят из строго положительных значений. К Вам приходят коллеги и говорят, что ваша модель начала выдавать отрицательные значения. Что сломалось, как починить?","Причина отрицательных предсказаний может быть связана с переобучением модели или с использованием методов регуляризации, таких как L2-регуляризация, которые могли сместить предсказания в отрицательную область. Чтобы исправить это, можно попробовать следующие действия:

Перепроверить данные: Убедиться, что все входные признаки и целевые переменные имеют ожидаемые диапазоны значений.

Перепроверить гиперпараметры модели: Настроить гиперпараметры модели, такие как глубина деревьев, количество деревьев и параметры регуляризации, чтобы улучшить обобщающую способность модели.

Применить постобработку: Применить постобработку к предсказаниям модели, например, использование функции ReLU (Rectified Linear Unit) для обрезки отрицательных значений или простое обнуление отрицательных значений.",,,
Решающие деревья и их ансамбли (лес и бустинг),"Когда мы добавляем дерево в случайный лес, как они изменяются?","При добавлении нового дерева в случайный лес каждое дерево остаётся независимым от других. Новое дерево обучается на случайной подвыборке данных и случайном подмножестве признаков, что увеличивает разнообразие моделей в лесу и может улучшить обобщающую способность модели за счёт снижения разброса предсказаний.",,Wildberries,
Решающие деревья и их ансамбли (лес и бустинг),"Когда увеличиваем глубину деревьев в случайном лесе, как изменяются?","При увеличении глубины деревьев в случайном лесе каждое дерево становится более способным к детализированию данных и может лучше подстраиваться под обучающие данные, что снижает смещение модели. Однако это также может привести к увеличению разброса и потенциальному переобучению, если глубина слишком велика.",,Wildberries,
Решающие деревья и их ансамбли (лес и бустинг),Какие деревья лучше брать в градиентном бустинге и случайном лесе: низкие или высокие?,"Градиентный бустинг: Лучше использовать низкие деревья (слабые модели), обычно с глубиной от 3 до 8 уровней, чтобы предотвратить переобучение и обеспечить корректирующую способность каждого дерева.

Случайный лес: Обычно используются высокие деревья (глубокие), так как метод бэггинга снижает риск переобучения. Глубокие деревья позволяют полностью обучиться на каждом подмножестве данных.",Стажер,Сбер,
Решающие деревья и их ансамбли (лес и бустинг),Как поведет себя бустинг если удалить первое/последнее дерево?,"Удаление первого дерева: Вероятно, значительно ухудшит производительность модели, так как первое дерево задаёт начальный базовый уровень предсказаний, на котором базируется всё дальнейшее улучшение.

Удаление последнего дерева: Может немного ухудшить производительность, так как последнее дерево обычно вносит менее значительные коррекции. Однако общее качество модели останется относительно стабильным.",Стажер,Сбер,
Статистика,МНК (Метод Наименьших Квадратов)-Что это такое?,"Метод наименьших квадратов (МНК) — это статистический метод для оценки параметров линейной регрессии, минимизирующий сумму квадратов разностей между наблюдаемыми значениями и значениями, предсказанными моделью. МНК даёт оптимальные оценки параметров в условиях гауссовского шума.",,,
Статистика,ММП (Метод Максимального Правдоподобия)-Что это такое?,"Метод максимального правдоподобия (ММП) — это метод оценки параметров статистической модели, который максимизирует правдоподобие наблюдаемых данных, исходя из выбранной модели. ММП широко используется из-за его хороших свойств, таких как состоятельность и асимптотическая нормальность.",,,
Узкий профиль - Анализ временных рядов / TSA,Приведите метрики для прогнозирования временных рядов,"Средняя абсолютная ошибка (MAE)
Среднеквадратичная ошибка (MSE)
Среднеквадратичный логарифмический корень ошибки (RMSLE)
Средняя абсолютная процентная ошибка (MAPE)
Симметричная средняя абсолютная процентная ошибка (sMAPE)
Коэффициент детерминации (R²)
Кросс-корреляция",,,
Узкий профиль - Анализ временных рядов / TSA,Обучения временных рядов-Как происходит обучение на временных рядах,"Обучение на временных рядах включает использование исторических временных данных для построения модели, которая прогнозирует будущие значения. Основные этапы:

Подготовка данных: Временные данные делятся на обучающую, валидационную и тестовую выборки с учётом временного порядка.

Создание признаков: Генерация признаков, таких как лаговые значения, скользящие средние и сезонные индикаторы.

Выбор модели: Подбор подходящей модели, такой как ARIMA, экспоненциальное сглаживание, LSTM или трансформеры.

Обучение модели: Настройка параметров модели на обучающих данных для минимизации функции потерь.

Валидация модели: Оценка точности модели на валидационной выборке и корректировка параметров.

Тестирование и внедрение: Применение модели к тестовым данным и её использование для реальных прогнозов.",,,
Узкий профиль - Анализ временных рядов / TSA,Как сделать кросс-валидацию на времянных рядах,"Для временных рядов стандартная кросс-валидация не подходит из-за временной зависимости данных. Вместо этого используется метод скользящего окна или прерывистой кросс-валидации:

Скользящее окно: Данные делятся на временные окна, где каждая итерация сдвигает тренировочный и тестовый периоды вперёд. Например, первые 70% данных для обучения и следующие 10% для теста, затем сдвиг на один период.

Прерывистая кросс-валидация (Time Series Split): Данные разделяются на последовательные временные сегменты, где каждый сегмент обучает модель, используя только предыдущие данные.",,,
Узкий профиль - Обработка естественного языка / NLP,"Правда ли, что позиционное кодирование критически необходимо трансформерам?","Да, позиционное кодирование критически необходимо для трансформеров, так как они не имеют встроенной способности учитывать порядок последовательностей, как, например, рекуррентные сети. Позиционное кодирование добавляет информацию о положении элементов в последовательности, позволяя трансформерам учитывать временную зависимость данных при обработке.",Junior NLP,MTS AI,https://datascience.stackexchange.com/questions/103224/why-do-transformers-need-positional-encodings
Узкий профиль - Обработка естественного языка / NLP,Опишите бейзлайн обучения БЕРТа для бинарной классификации,"Предобработка данных: Используйте предобученную модель BERT для извлечения скрытых представлений текстов. Тексты токенизируются и кодируются, включая специальные токены, такие как [CLS] для начала последовательности и [SEP] для разделения предложений.

Модель: BERT обрезается до [CLS] токена последнего слоя, который содержит информацию обо всей последовательности.

Классификатор: На выходе BERT добавляется полносвязный слой с сигмоидной активацией для предсказания вероятности принадлежности к одному из классов.

Функция потерь: Используется бинарная кросс-энтропия для оптимизации весов.

Обучение: Настраиваются верхние слои BERT и классификатор, используя заранее определённый датасет с метками классов.

Тонкая настройка (Fine-tuning): Обучение модели на небольшом количестве эпох с использованием небольшого коэффициента обучения, чтобы сохранить выученные BERTом представления.",Junior NLP,Сбер,
Узкий профиль - Обработка естественного языка / NLP,"Что будет, если на вход БЕРТу/ЛЛМ придет неизвестное слово?","Если на вход BERTу или другой LLM (Large Language Model) поступит неизвестное слово, то оно будет токенизировано на более мелкие части с помощью BPE токенизатора. Если слово полностью неизвестно, его части могут быть представлены в виде подслов (subwords) или отдельных символов, которые имеют известные представления в модели. Это позволяет моделям эффективно работать даже с незнакомыми словами, используя комбинацию известных токенов.",Junior NLP,Сбер,
Узкий профиль - Обработка естественного языка / NLP,Как работает BPE токенизатор (Byte Pair Encoding Tokenizer)?,"BPE токенизатор работает путём итеративного объединения часто встречающихся пар символов в тексте для создания новых токенов. Процесс состоит из следующих шагов:

Инициализация: Каждое слово разбивается на отдельные символы.

Поиск частотных пар: Определяются иерархически наиболее часто встречающиеся пары символов или подслов.

Объединение пар: Частотные пары символов объединяются в один токен, и словарь токенов обновляется.

Повторение: Процесс повторяется до достижения заданного количества токенов или пока нет новых частых пар для объединения.

Это позволяет BPE эффективно сокращать текст и справляться с неизвестными словами через разбиение на подслова.",Junior NLP,MTS AI,
Узкий профиль - Обработка естественного языка / NLP,Почему Self-Attention имеет приставку Self?,"Self-Attention имеет приставку ""Self"" потому, что каждый элемент последовательности (например, слово в предложении) оценивает своё внимание по отношению ко всем другим элементам этой же последовательности, включая себя. Это позволяет модели захватывать контекстную информацию и зависимости между словами независимо от их удалённости в тексте. Self-Attention вычисляет матрицу внимания на основе внутреннего контекста последовательности, что способствует учёту как локальных, так и глобальных зависимостей.",Стажер,АИРИ,
Узкий профиль - Обработка естественного языка / NLP,В чем роль маски внимания (Attention Mask)?,"Маска внимания (Attention Mask) используется для исключения ненужных частей последовательности из участия в процессе внимания. Она применяется, чтобы:

Обработка разной длины предложений: Игнорировать токены-паддинги, которые добавляются для выравнивания длины последовательностей.

Скрытие информации: В задачах вроде машинного перевода маска скрывает будущие токены от участия в текущем вычислении, чтобы избежать утечки информации (например, при генерации текста).

Улучшение эффективности: Снижает вычислительную нагрузку, исключая ненужные элементы, которые не должны участвовать в расчёте внимания.",Junior NLP,MTS AI,
Узкий профиль - Обработка естественного языка / NLP,Как обучается BERT в задаче NER?,"В задаче распознавания именованных сущностей (NER) BERT используется следующим образом:

Подготовка данных: Текст метится с указанием сущностей, такие как ""PERSON"", ""LOCATION"", и т.д.

Токенизация: Данные проходят через BERT токенизатор, преобразующий текст в последовательности токенов с помощью специальных токенов, таких как [CLS] и [SEP].

Модель: Векторные представления каждого токена на выходе BERT используются для классификации каждого токена в один из классов сущностей.

Классификатор: На выходные представления BERT добавляется линейный слой для предсказания классов сущностей.

Функция потерь: Используется кросс-энтропийная функция потерь для мультиклассовой классификации.

Тонкая настройка (Fine-tuning): BERT и добавленный слой обучаются на размеченных данных NER, чтобы выделять именованные сущности с высокой точностью.",Junior NLP,MTS AI,
Узкий профиль - Обработка естественного языка / NLP,Что такое word2vec?,"Word2vec — это метод для представления слов в виде векторных представлений в многомерном пространстве. Он обучается на больших корпусах текстов и создаёт векторы, которые сохраняют семантические и синтаксические отношения между словами. Существует два основных подхода в Word2vec:

CBOW (Continuous Bag of Words): Предсказывает текущее слово, исходя из окружающих его слов.

Skip-Gram: Предсказывает окружающие слова, исходя из текущего слова.

Word2vec позволяет захватывать семантические отношения и аналогии между словами в плотных векторах.",Junior NLP,MTS AI,
Узкий профиль - Обработка естественного языка / NLP,Что такое метрика ROUGE? Как считается?,"ROUGE (Recall-Oriented Understudy for Gisting Evaluation) — это метрика оценки качества машинного перевода и реферативного обобщения текстов. Она сравнивает текст модели с эталонным текстом и оценивает:

ROUGE-N: Сравнивает n-граммы в предсказанном и эталонном текстах.

ROUGE-L: Основывается на длиннейшей общей подпоследовательности (Longest Common Subsequence, LCS).

ROUGE-W: Использует взвешенную версию LCS.

ROUGE-S: Оценивает совпадение пропущенных биграмм.

ROUGE оценивает как точность, так и полноту (recall), предоставляя несколько вариантов подсчёта совпадений между сгенерированным и эталонным текстами.",Junior NLP,MTS AI,
Узкий профиль - Обработка естественного языка / NLP,Как работает Beam Search?,"Beam Search — это метод поиска оптимальных последовательностей в задачах генерации текста, таких как машинный перевод. Он работает следующим образом:

Инициализация: Начинается с начальной последовательности (например, стартового токена).

Расширение: Для каждой возможной последовательности генерируются все возможные продолжения, оцениваемые с точки зрения вероятности.

Отбор: Из всех возможных продолжений выбирается фиксированное количество (ширина луча, beam width) наиболее вероятных путей для дальнейшего расширения.

Итерация: Процесс повторяется, пока не будет достигнута заданная длина последовательности или специальный конечный токен.

Выбор оптимальной последовательности: В конце выбирается наиболее вероятная последовательность среди всех рассмотренных путей.

Beam Search эффективно балансирует между исследованием пространства поиска и вычислительными затратами, позволяя находить более вероятные последовательности по сравнению с жадным поиском.",Junior NLP,MTS AI,
Узкий профиль - Разработка систем машинного обучения / ML System Design,"Этапы построения модели машинного обучения-1. Расскажите как вы будете собирать данные, создавать и выводить в продакшен","Сбор данных:

Определение цели: Понимание задачи и требуемых данных.
Источники данных: Определение доступных источников, таких как базы данных, API, веб-скрапинг и публичные наборы данных.
Очистка данных: Удаление выбросов, обработка пропусков и аномалий, стандартизация форматов.
Разведочный анализ (EDA): Анализ структуры данных, визуализация распределений, корреляций и выявление зависимостей.
Создание модели:

Разделение данных: Разделение на обучающую, валидационную и тестовую выборки.
Предобработка данных: Нормализация, кодирование категориальных признаков, создание новых признаков.
Выбор модели: Подбор подходящей модели или ансамбля моделей в зависимости от задачи.
Обучение модели: Настройка модели на обучающих данных, оптимизация гиперпараметров с использованием валидационной выборки.
Оценка модели: Использование метрик для оценки производительности на валидационных и тестовых данных.
Тонкая настройка: Доработка модели и повторение шагов обучения и оценки до достижения удовлетворительного результата.
Вывод в продакшен:

Тестирование: Проверка модели на стабильность и производительность в различных условиях.
Документация: Создание документации, описывающей использование модели, её ограничения и метрики качества.
Интеграция: Внедрение модели в рабочие процессы или системы, создание API для доступа к модели.
Мониторинг: Постоянный мониторинг производительности модели на реальных данных, выявление дрейфа данных и необходимость в переобучении.
Поддержка и обновление: Регулярное обновление модели на новых данных для улучшения качества и адаптации к изменениям в данных.",,,